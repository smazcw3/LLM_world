{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed1bdc8",
   "metadata": {},
   "source": [
    "## Chains\n",
    "\n",
    "The chains are responsible for creating an end-to-end pipeline for using the language models. They will join the model, prompt, memory, parsing output, and debugging capability and provide an easy-to-use interface. \n",
    "\n",
    "A chain will \n",
    "+ receive the user’s query as an input, \n",
    "+ process the LLM’s response, and lastly,\n",
    "+ return the output to the user.\n",
    "\n",
    "It is possible to design a custom pipeline by inheriting the Chain class. For example, the LLMChain is the simplest form of chain in LangChain, inheriting from the Chain parent class. We will start by going through ways to invoke this class and follow it by looking at adding different functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd05af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken scikit-learn deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ecff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c45657",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418d6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e5c7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de366aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\\n\\nSynthetic'},\n",
       " {'text': '\\n\\nCleverness'},\n",
       " {'text': '\\n\\nandroid'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It is also possible to use the .apply() method to pass multiple inputs at once and receive a list for each input. \n",
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ee4f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nCleverness', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nandroid', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 42, 'prompt_tokens': 33, 'completion_tokens': 9}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('08cddb52-73e8-4410-864e-3b03e915c01c')), RunInfo(run_id=UUID('39a12341-8a70-498b-9589-71d02cc8b92d')), RunInfo(run_id=UUID('5448796e-b783-4d12-bb3a-ead4c9d48344'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The .generate() method will return an instance of LLMResult, which provides more information. For example, the finish_reason key indicates the reason behind the stop of the generation process. It could be stopped, meaning the model decided to finish or reach the length limit. There is other self-explanatory information like the number of total used tokens or the used model.\n",
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151c9625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSupporter'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The next method we will discuss is .predict() (which could be used interchangeably with .run()).\n",
    "#Its best use case is to pass multiple inputs for a single prompt. However, it is possible to use it with one input variable as well. The following prompt will pass both the word we want a substitute for and the context the model must consider.\n",
    "prompt_template = \"Looking at the context of '{context}'. What is an appropriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"fan\", context=\"object\")\n",
    "# or llm_chain.run(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6779ad3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSupporter'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"humans\")\n",
    "# or llm_chain.run(word=\"fan\", context=\"humans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f954f7",
   "metadata": {},
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180afc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synthetic',\n",
       " 'man-made',\n",
       " 'fake',\n",
       " 'imitation',\n",
       " 'simulated',\n",
       " 'faux',\n",
       " 'fabricated',\n",
       " 'counterfeit',\n",
       " 'ersatz',\n",
       " 'false',\n",
       " 'pretend',\n",
       " 'mock',\n",
       " 'spurious',\n",
       " 'bogus',\n",
       " 'phony',\n",
       " 'contrived',\n",
       " 'manufactured',\n",
       " 'unnatural',\n",
       " 'inauthentic']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, output_parser=output_parser, input_variables=[]),\n",
    "    output_parser=output_parser)\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd95522",
   "metadata": {},
   "source": [
    "## Conversational Chain (Memory)\n",
    "\n",
    "Depending on the application, memory is the next component that will complete a chain. LangChain provides a ConversationalChain to track previous prompts and responses using the ConversationalBufferMemory class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026f757e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Synthetic, simulated, man-made, fake, imitation, fabricated, manufactured, engineered, virtual, computer-generated, digital, non-natural, inauthentic, contrived, ersatz, pseudo, phony, false, unreal, pretend, counterfeit, spurious, bogus, sham, feigned, deceptive, fraudulent, specious, fallacious, misleading, delusive, illusory, fictitious, make-believe, imaginary, unreal, fictive, fanciful, fantastic, mythical, legendary, mythological, fabricated, created, contrived, invented, made-up, unreal, imaginary, fictional, mythical, legendary, mythological, fanciful, fantastic, make-believe, pretend, simulated, virtual, computer-generated, digital, non-natural, inauthentic, ersatz, pseudo, phony, false, counterfeit, spurious, bogus, sham, feigned, deceptive, fraudulent, specious, fallacious, misleading, delusive, illusory, fictitious, make-believe, imaginary, unreal, fictive, fanciful, fantastic, mythical, legendary, mythological, fabricated, created, contrived, invented, made-up, unreal, imaginary, fictional, mythical, legendary, mythological, fanciful, fantastic,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"List all possible words as substitute for 'artificial' as comma separated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44608d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Imagined, conceptual, conceptualized, theoretical.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, we can ask it to return the following four replacement words. It uses the memory to find the next options.\n",
    "conversation.predict(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b7f99",
   "metadata": {},
   "source": [
    "## Sequential Chain\n",
    "\n",
    "Another helpful feature is using a sequential chain that concatenates multiple chains into one. The following code shows a sample usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b08e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poet\n",
    "poet_template: str = \"\"\"You are an American poet, your job is to come up with\\\n",
    "poems based on a given theme.\n",
    "\n",
    "Here is the theme you have been asked to generate a poem on:\n",
    "{input}\\\n",
    "\"\"\"\n",
    "\n",
    "poet_prompt_template: PromptTemplate = PromptTemplate(\n",
    "    input_variables=[\"input\"], template=poet_template)\n",
    "\n",
    "# creating the poet chain\n",
    "poet_chain: LLMChain = LLMChain(\n",
    "    llm=llm, output_key=\"poem\", prompt=poet_prompt_template)\n",
    "\n",
    "# critic\n",
    "critic_template: str = \"\"\"You are a critic of poems, you are tasked\\\n",
    "to inspect the themes of poems. Identify whether the poem includes romantic expressions or descriptions of nature.\n",
    "\n",
    "Your response should be in the following format, as a Python Dictionary.\n",
    "poem: this should be the poem you received \n",
    "Romantic_expressions: True or False\n",
    "Nature_descriptions: True or False\n",
    "\n",
    "Here is the poem submitted to you:\n",
    "{poem}\\\n",
    "\"\"\"\n",
    "\n",
    "critic_prompt_template: PromptTemplate = PromptTemplate(\n",
    "    input_variables=[\"poem\"], template=critic_template)\n",
    "\n",
    "# creating the critic chain\n",
    "critic_chain: LLMChain = LLMChain(\n",
    "    llm=llm, output_key=\"critic_verified\", prompt=critic_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05e94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[poet_chain, critic_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3746dd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poem: Nature's Beauty\n",
      "Romantic_expressions: False\n",
      "Nature_descriptions: True\n"
     ]
    }
   ],
   "source": [
    "# Run the poet and critic chain with a specific theme\n",
    "theme: str = \"the beauty of nature\"\n",
    "review = overall_chain.run(theme)\n",
    "\n",
    "# Print the review to see the critic's evaluation\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c1aafe",
   "metadata": {},
   "source": [
    "### Debug\n",
    "\n",
    "It is possible to trace the inner workings of any chain by setting the verbose argument to True. As you can see in the following code, the chain will return the initial prompt and the output. The output depends on the application. It may contain more information if there are more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2cd6e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1. Synthetic\\n2. Man-made\\n3. Faux\\n4. Imitation\\n5. Simulated\\n6. Fake\\n7. Replicated\\n8. Manufactured\\n9. Counterfeit\\n10. Mock\\n11. Pretend\\n12. False\\n13. Inauthentic\\n14. Unnatural\\n15. Fabricated'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a2fbc",
   "metadata": {},
   "source": [
    "## Custom Chain\n",
    "\n",
    "The LangChain library has several predefined chains for different applications like Transformation Chain, LLMCheckerChain, LLMSummarizationCheckerChain, and OpenAPI Chain, which all share the same characteristics mentioned in previous sections. It is also possible to define your chain for any custom task. In this section, we will create a chain that returns a word's meaning and then suggests a replacement.\n",
    "\n",
    "It starts by defining a class that inherits most of its functionalities from the Chain class. Then, the following three methods must be declared depending on the use case. The `input_keys` and `output_keys` methods let the model know what it should expect, and a `_call` method runs each chain and merges their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db3c4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc4131cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output:\n",
      "\n",
      "\n",
      "Artificial means something that is made or produced by humans, rather than occurring naturally. It can also refer to something that is not genuine or authentic, but rather created or imitated to appear real.\n",
      "\n",
      "Synthetic\n"
     ]
    }
   ],
   "source": [
    "#Then, we will declare each chain individually using the LLMChain class.\n",
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "#Lastly, we call our custom chain ConcatenateChain to merge the results of the chain_1 and chain_2\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568892a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
