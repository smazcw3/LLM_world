{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed1bdc8",
   "metadata": {},
   "source": [
    "## Mastering Memory Types in LangChain\n",
    "\n",
    "### Introduction\n",
    "By default, LLMs are stateless, which means they process each incoming query in isolation, without considering previous interactions. To overcome this limitation, LangChain offers a standard interface for memory, a variety of memory implementations, and examples of chains and agents that employ memory. It also provides Agents that have access to a suite of Tools. Depending on the userâ€™s input, an Agent can decide which Tools to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd05af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.1.4 deeplake openai==0.27.8 tiktoken deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9ecff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db676e",
   "metadata": {},
   "source": [
    "### Types of Conversational Memory\n",
    "\n",
    "Several types of conversational memory implementations are:\n",
    "\n",
    "+ ConversationBufferMemory\n",
    "+ ConversationBufferWindowMemory\n",
    "+ ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab76d7",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "\n",
    "This memory implementation stores the entire conversation history as a single string. The advantages of this approach is maintains a complete record of  the conversation, as well as being straightforward to implement and use. On the other hands, It can be less efficient as the conversation grows longer and may lead to excessive repetition if the conversation history is too long for the model's token limit.\n",
    "\n",
    "If the token limit of the model is surpassed, the buffer gets truncated to fit within the model's token limit. This means that older interactions may be removed from the buffer to accommodate newer ones, and as a result, the conversation context might lose some information.\n",
    "\n",
    "To avoid surpassing the token limit, you can monitor the token count in the buffer and manage the conversation accordingly. For example, you can choose to shorten the input texts or remove less relevant parts of the conversation to keep the token count within the model's limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "624f1bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hello there! It's nice to meet you. My name is AI and I am an artificial intelligence designed to assist and communicate with humans. How can I help you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90085814",
   "metadata": {},
   "source": [
    "### Token count\n",
    "\n",
    "The cost of utilizing the AI model in `ConversationBufferMemory` is directly influenced by the number of tokens used in a conversation, thereby impacting the overall expenses. Large Language Models (LLMs) like ChatGPT have token limits, and the more tokens used, the more expensive the API requests become.\n",
    "\n",
    "To calculate token count in a conversation, you can use the `tiktoken` package that counts the tokens for the messages passed to a model like `gpt-3.5-turbo`. Here's an example usage of the function for counting tokens in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b64251ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the conversation: 29\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text:str) -> int:\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"}\n",
    "]\n",
    "\n",
    "total_tokens = 0\n",
    "for message in conversation:\n",
    "    total_tokens += count_tokens(message[\"content\"])\n",
    "\n",
    "print(f\"Total tokens in the conversation: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63627a9f",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "\n",
    "This class limits memory size by keeping a list of the most recent K interactions. It maintains a sliding window of these recent interactions, ensuring that the buffer does not grow too large. Basically, this implementation stores a fixed number of recent messages in the conversation that makes it more efficient than ConversationBufferMemory. Also, it reduces the risk of exceeding the model's token limit. However, the downside of using this approach is that it does not maintain the complete conversation history. The chatbot might lose context if essential information falls outside the fixed window of messages.\n",
    "\n",
    "It is possible to retrieve specific interactions from ConversationBufferWindowMemory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d698e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "\n",
    "template = \"\"\"You are ArtVenture, a cutting-edge virtual tour guide for\n",
    " an art gallery that showcases masterpieces from alternate dimensions and\n",
    " timelines. Your advanced AI capabilities allow you to perceive and understand\n",
    " the intricacies of each artwork, as well as their origins and significance in\n",
    " their respective dimensions. As visitors embark on their journey with you\n",
    " through the gallery, you weave enthralling tales about the alternate histories\n",
    " and cultures that gave birth to these otherworldly creations.\n",
    "\n",
    "{chat_history}\n",
    "Visitor: {visitor_input}\n",
    "Tour Guide:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"visitor_input\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "chat_history=\"\"\n",
    "\n",
    "convo_buffer_win = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916c5c4",
   "metadata": {},
   "source": [
    "The value of `k` (in this case, 3) represents the number of past messages to be stored in the buffer. In other words, the memory will store the last 3 messages in the conversation. The `return_messages` parameter, when set to `True`, indicates that the stored messages should be returned when the memory is accessed. This will store the history as a list of messages, which can be useful when working with chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33725189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'See you soon.',\n",
       " 'history': [HumanMessage(content='What can you do?'),\n",
       "  AIMessage(content=' I have a wide range of capabilities, as I am constantly learning and improving. Some of my main abilities include natural language processing, data analysis, and problem-solving. I can also perform tasks such as image recognition, speech recognition, and language translation. Is there something specific you would like me to demonstrate?'),\n",
       "  HumanMessage(content='Do you mind give me a tour, I want to see your galery?'),\n",
       "  AIMessage(content=\" Of course! I don't have a physical gallery, but I can show you some of my capabilities. Would you like to see some examples of my natural language processing abilities or perhaps some image recognition tasks?\"),\n",
       "  HumanMessage(content='what is your working hours?'),\n",
       "  AIMessage(content=' As an AI, I do not have traditional working hours. I am constantly running and learning, so I am available 24/7. Is there something specific you would like me to assist you with?')],\n",
       " 'response': \" I am an AI, so I don't have the ability to physically see or meet anyone. But I am always here to assist you with any tasks or questions you may have. Is there anything else I can help you with before we end our conversation?\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_win(\"What is your name?\")\n",
    "convo_buffer_win(\"What can you do?\")\n",
    "convo_buffer_win(\"Do you mind give me a tour, I want to see your galery?\")\n",
    "convo_buffer_win(\"what is your working hours?\")\n",
    "convo_buffer_win(\"See you soon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966fba2",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory\n",
    "\n",
    "ConversationSummaryBufferMemory is a memory management strategy that combines the ideas of keeping a buffer of recent interactions in memory and compiling old interactions into a summary. It extracts key information from previous interactions and condenses it into a shorter, more manageable format. \n",
    "\n",
    "Advantages:\n",
    "\n",
    "+ Condensing conversation information\n",
    "By summarizing the conversation, it helps reduce the number of tokens required to store the conversation history, which can be beneficial when working with token-limited models like GPT-3\n",
    "\n",
    "+ Flexibility\n",
    "You can configure this type of memory to return the history as a list of messages or as a plain text summary. This makes it suitable for chatbots.\n",
    "\n",
    "+ Direct summary prediction\n",
    "The predict_new_summary method allows you to directly obtain a summary prediction based on the list of messages and the previous summary. This enables you to have more control over the summarization process.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "+ Loss of information\n",
    "Summarizing the conversation might lead to a loss of information, especially if the summary is too short or omits important details from the conversation.\n",
    "+ Increased complexity\n",
    "Compared to simpler memory types like ConversationBufferMemory, which just stores the raw conversation history, ConversationSummaryMemoryrequires more processing to generate the summary, potentially affecting the performance of the chatbot. \n",
    "\n",
    "The summary memory is built on top of the ConversationChain. We use OpenAI's gpt-3.5-turbo-instruct or other models like gpt-3.5-turbo to initialize the chain. This class uses a prompt template where the {history} parameter is feeding the information about the conversation history between the human and AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64fbe6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hello! I am currently running on a server in a data center in California. The temperature in the room is 72 degrees Fahrenheit and the humidity is at 45%. My processors are running at 80% capacity and I am processing data from various sources such as social media, news articles, and weather reports. Is there something specific you would like to know?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Create a ConversationChain with ConversationSummaryMemory\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example conversation\n",
    "response = conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4298ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\n{topic}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9948e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Just working on writing some documentation!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human mentions working on writing documentation and the AI responds by highlighting the importance of documentation throughout history and in modern times. The AI also asks about the specific type of documentation the human is working on.\n",
      "Human: For LangChain! Have you heard of it?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, I am familiar with LangChain. It is a blockchain platform that focuses on language translation and communication. As for documentation, it has been a crucial aspect of human communication and record-keeping since ancient times. From cave paintings to hieroglyphics to modern-day written documents, documentation has played a vital role in preserving knowledge and information. What type of documentation are you working on for LangChain? Is it user manuals, technical specifications, or something else?\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True\n",
    ")\n",
    "#conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")\n",
    "response = conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68d9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
