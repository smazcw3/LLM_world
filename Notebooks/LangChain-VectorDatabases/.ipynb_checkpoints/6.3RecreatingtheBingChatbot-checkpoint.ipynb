{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed1bdc8",
   "metadata": {},
   "source": [
    "## Recreating the Bing Chatbot\n",
    "\n",
    "### Introduction\n",
    "While the Large Language Models (LLMs) possess impressive capabilities, they have certain limitations that can present challenges when deploying them in a production environment. The hallucination problem makes them answer certain questions wrongly with high confidence. This issue can be attributed to various factors, one of which is that their training process has a cut-off date. So, these models do not have access to events preceding that date.\n",
    "\n",
    "A workaround approach is to present the required information to the model and leverage its reasoning capability to find/extract the answer. Furthermore, it is possible to present the top-matched results a search engine returns as the context for a user’s query.\n",
    "\n",
    "This lesson will explore the idea of finding the best articles from the Internet as the context for a chatbot to find the correct answer. We will use LangChain’s integration with Google Search API and the Newspaper library to extract the stories from search results. This is followed by choosing and using the most relevant options in the prompt.\n",
    "\n",
    "Notice that the same pipeline could be done with the Bing API, but we’ll use the Google Search API in this project because it is used in other lessons of this course, thus avoiding creating several keys for the same functionality. Please refer to the following tutorial (or Bing Web Search API for direct access) on obtaining the Bing Subscription Key and using the LangChain Bing search wrapper.\n",
    "\n",
    "<img src=\"../../images/bing_chatbot.png\" width=400 height=100 />\n",
    "\n",
    "The user query is used to extract relevant articles using a search engine (e.g. Bing or Google Search), which are then split into chunks. We then compute the embeddings of each chunk, rank them by cosine similarity with respect to the embedding of the query, and put the most relevant chunks into a prompt to generate the final answer, while also keeping track of the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd05af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.1.4 deeplake openai==0.27.8 tiktoken deeplake google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ecff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f759d85",
   "metadata": {},
   "source": [
    "### Get Search Results\n",
    "\n",
    "This section uses LangChain’s `GoogleSearchAPIWrapper` class to receive search results. It works in combination with the `Tool` class that presents the utilities for agents to help them interact with the outside world. In this case, creating a tool out of any function, like `top_n_results` is possible. The API will return the page’s title, URL, and a short description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230418f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast & Furious - Wikipedia\n",
      "https://en.wikipedia.org/wiki/Fast_%26_Furious\n",
      "It began the original tetralogy of films focused on illegal street racing, which culminated in the film Fast & Furious (2009). The series moved towards heists ...\n",
      "--------------------------------------------------\n",
      "Fast & Furious movies in order | chronological and release order ...\n",
      "https://www.radiotimes.com/movies/fast-and-furious-order/\n",
      "Mar 22, 2023 ... Fast & Furious Presents: Hobbs & Shaw (2019); F9 (2021); Fast and Furious 10 (2023). Tokyo Drift also marks the first appearance of Han Lue, a ...\n",
      "--------------------------------------------------\n",
      "Fast and Furious Movies in Order Chronologically and by Release ...\n",
      "https://movieweb.com/fast-and-furious-movies-in-order/\n",
      "Jul 24, 2023 ... The Fate of the Furious (2017). FateoftheFurious (1) Universal Pictures. Once again, Dominic finds himself getting ...\n",
      "--------------------------------------------------\n",
      "Here's How to Watch Every Fast and Furious Movie In Order ...\n",
      "https://www.menshealth.com/entertainment/a36716650/fast-and-furious-movies-in-order/\n",
      "Sep 18, 2023 ... Furious 7 is the last movie in the Fast and Furious series to feature the late Paul Walker (as Brian O'Conner), and the movie sends his ...\n",
      "--------------------------------------------------\n",
      "How to Watch Fast and Furious Movies in Chronological Order - IGN\n",
      "https://www.ign.com/articles/fast-and-furious-movies-in-order\n",
      "F9 (2021) ... The series' most recent film before Fast X, once again directed by Justin Lin, pits Dom and the crew against Jakob Toretto, Dom's estranged brother ...\n",
      "--------------------------------------------------\n",
      "How many 'Fast & Furious' movies are there? Here's the list in order.\n",
      "https://www.usatoday.com/story/entertainment/movies/2022/07/29/fast-and-furious-movies-order-of-release/10062943002/\n",
      "Jul 29, 2022 ... There are currently 10 films in the main \"Fast and Furious\" franchise. The latest installment, \"Fast X,\" released on May 19, 2023. There are ...\n",
      "--------------------------------------------------\n",
      "FAST X | Final Trailer - YouTube\n",
      "https://www.youtube.com/watch?v=eoOaKN4qCKw\n",
      "May 15, 2023 ... ... last 12 years masterminding a plan to make ... film also features an extraordinary new cast ... Fast X, the tenth film in the Fast & Furious ...\n",
      "--------------------------------------------------\n",
      "Furious Six was the last good Fast and Furious. : r/movies\n",
      "https://www.reddit.com/r/movies/comments/kkmvi3/furious_six_was_the_last_good_fast_and_furious/\n",
      "Dec 26, 2020 ... Furious Six was the last good Fast and Furious. I know a lot of people hate pretty much every film in the franchise that isn't the original ...\n",
      "--------------------------------------------------\n",
      "Fast & Furious Movies In Order: How to Watch Fast Saga ...\n",
      "https://editorial.rottentomatoes.com/guide/fast-furious-movies-in-order/\n",
      "After that, hop to franchise best Furious 7. Follow it up with The Fate of the Furious and spin-off Hobbs & Shaw and then the latest: F9 and Fast X. See below ...\n",
      "--------------------------------------------------\n",
      "Cool TRX in the Fast and Furious movie | RAM TRX Forum\n",
      "https://www.ram-trx.com/threads/cool-trx-in-the-fast-and-furious-movie.17291/\n",
      "May 29, 2023 ... The current front fenders are carbon fiber and are approximately 5.5inch wider per side. The track width of truck is 96inch wide. It looks ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "TOP_N_RESULTS = 10\n",
    "\n",
    "def top_n_results(query):\n",
    "    return search.results(query, TOP_N_RESULTS)\n",
    "\n",
    "tool = Tool(\n",
    "    name = \"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top_n_results\n",
    ")\n",
    "\n",
    "query = \"What is the latest fast and furious movie?\"\n",
    "\n",
    "results = tool.run(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"title\"])\n",
    "    print(result[\"link\"])\n",
    "    print(result[\"snippet\"])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a1008",
   "metadata": {},
   "source": [
    "Now, we use the `results` variable’s `link` key to download and parse the contents. The newspaper library takes care of everything. However, it might be unable to capture some contents in certain situations, like anti-bot mechanisms or having a file as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89295b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        article = newspaper.Article(result[\"link\"])\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if len(article.text) > 0:\n",
    "            pages_content.append({ \"url\": result[\"link\"], \"text\": article.text })\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9424b22",
   "metadata": {},
   "source": [
    "### Process the Search Results\n",
    "We now have the top 10 results from the Google search. (Honestly, who looks at Google’s second page?) However, it is not efficient to pass all the contents to the model because of the following reasons:\n",
    "\n",
    "+ The model’s context length is limited.\n",
    "+ It will significantly increase the cost if we process all the search results.\n",
    "+ In almost all cases, they share similar pieces of information.\n",
    "So, let’s find the most relevant results, \n",
    "\n",
    "Incorporating the LLMs embedding generation capability will enable us to find contextually similar content. It means converting the text to a high-dimensionality tensor that captures meaning. The cosine similarity function can find the closest article with respect to the user’s question.\n",
    "\n",
    "It starts by splitting the texts using the `RecursiveCharacterTextSplitter` class to ensure the content lengths are inside the model’s input length. The `Document` class will create a data structure from each chunk that enables saving metadata like URL as the source. The model can later use this data to know the content’s location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6847a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=100)\n",
    "\n",
    "docs = []\n",
    "for d in pages_content:\n",
    "    chunks = text_splitter.split_text(d[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        new_doc = Document(page_content=chunk, metadata={ \"source\": d[\"url\"] })\n",
    "        docs.append(new_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d1158",
   "metadata": {},
   "source": [
    "The subsequent step involves utilizing the OpenAI API's `OpenAIEmbeddings` class, specifically the `.embed_documents()` method for search results and the `.embed_query()` method for the user's question, to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9803b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "docs_embeddings = embeddings.embed_documents([doc.page_content for doc in docs])\n",
    "query_embedding = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a83f9",
   "metadata": {},
   "source": [
    "Lastly, the `get_top_k_indices` function accepts the content and query embedding vectors and returns the index of top K candidates with the highest cosine similarities to the user's request. Later, we use the indexes to retrieve the best-fit documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7301d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_top_k_indices(list_of_doc_vectors, query_vector, top_k):\n",
    "    # convert the lists of vectors to numpy arrays\n",
    "    list_of_doc_vectors = np.array(list_of_doc_vectors)\n",
    "    query_vector = np.array(query_vector)\n",
    "\n",
    "    # compute cosine similarities\n",
    "    similarities = cosine_similarity(query_vector.reshape(1, -1), list_of_doc_vectors).flatten()\n",
    "\n",
    "    # sort the vectors based on cosine similarity\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "    # retrieve the top K indices from the sorted list\n",
    "    top_k_indices = sorted_indices[:top_k]\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "top_k = 2\n",
    "best_indexes = get_top_k_indices(docs_embeddings, query_embedding, top_k)\n",
    "best_k_documents = [doc for i, doc in enumerate(docs) if i in best_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df567ff",
   "metadata": {},
   "source": [
    "### Chain with Source\n",
    "Finally, we used the selected articles in our prompt (using the `stuff` method) to assist the model in finding the correct answer. LangChain provides the `load_qa_with_sources_chain()` chain, which is designed to accept a list of `input_documents` as a source of information and a `question` argument which is the user’s question. The final part involves preprocessing the model’s response to extract its answer and the sources it utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd281098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The latest Fast and Furious movie is \"Fast and Furious Presents: Hobbs & Shaw\" (2019).\n",
      "Sources: https://www.menshealth.com/entertainment/a36716650/fast-and-furious-movies-in-order/\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "chain = load_qa_with_sources_chain(OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0), chain_type=\"stuff\")\n",
    "\n",
    "response = chain({\"input_documents\": best_k_documents, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "response_text, response_sources = response[\"output_text\"].split(\"SOURCES:\")\n",
    "response_text = response_text.strip()\n",
    "response_sources = response_sources.strip()\n",
    "\n",
    "print(f\"Answer: {response_text}\")\n",
    "print(f\"Sources: {response_sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afd37e",
   "metadata": {},
   "source": [
    "The use of search results helped the model find the correct answer, even though it never saw it before during the training stage. The question and answering chain with source also provides information regarding the sources utilized by the model to derive the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b448170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
