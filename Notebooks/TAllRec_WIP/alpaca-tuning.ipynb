{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40b9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eded753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.37.2 bitsandbytes==0.42.0 accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eadb633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404ce0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the current path of the execution\n",
    "import sys\n",
    "import os\n",
    "cwd = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.append(cwd)\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d5302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smaz-home/anaconda3/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Transformers version: 4.37.2\n",
      "BitsAndBytes version: 0.42.0\n",
      "Accelerate version: 0.27.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"BitsAndBytes version: {bitsandbytes.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b3758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data point:\n",
      "{'instruction': 'Given the user\\'s preference and unpreference, identify whether the user will like the target book by answering \"Yes.\" or \"No.\".', 'input': 'User Preference: \"The End of Enemies (Briggs Tanner Novels)\" written by Grant Blackwood, \"Q Is for Quarry\" written by Sue Grafton\\nUser Unpreference: \"ICEFIRE\" written by Judith Reeves-Stevens\\nWhether the user will like the target book \"Specter of the Past: Star Wars (Star Wars (Bantam Books (Firm) : Unnumbered).)\" written by Timothy Zahn?', 'output': 'Yes.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_dir = 'llm_datasets/book_crossing'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(os.path.join(data_dir, \"train.json\"), 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "# Print a sample to verify the format\n",
    "print(\"Sample data point:\")\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca25f4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smaz-home/anaconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8ba58d1e534afd9133d42a426a9949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model ID\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  #This tells the tokenizer to add padding tokens to the right side of the sequence\n",
    "\n",
    "# Configure model loading based on device\n",
    "if device == \"cuda\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_quant_type=\"nf8\",\n",
    "        bnb_8bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0708daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Combine instruction, input, and output\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{examples['instruction']}\n",
    "\n",
    "### Input:\n",
    "{examples['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    response = examples['output']\n",
    "    \n",
    "    # Tokenize prompt and response\n",
    "    prompt_ids = tokenizer(prompt, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
    "    response_ids = tokenizer(response, truncation=False, add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    # Combine them and truncate if needed\n",
    "    input_ids = prompt_ids + response_ids + [tokenizer.eos_token_id]\n",
    "    \n",
    "    ## 1. Most transformer models have a maximum sequence length\n",
    "    # For LLaMA-2, it's typically 512 tokens\n",
    "    #Consider this as managing sequence length to fit the model's maximum context window\n",
    "    if len(input_ids) > 512:\n",
    "        input_ids = input_ids[:511] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Create labels (same as input_ids for causal LM)\n",
    "    # -100 is hardcoded in PyTorch and the transformers library as a special \"ignore_index\" value when calculating loss\n",
    "    labels = [-100] * len(prompt_ids) + response_ids + [tokenizer.eos_token_id]\n",
    "    if len(labels) > 512:\n",
    "        labels = labels[:511] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    # Pad everything to max_length\n",
    "    padding_length = 512 - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        # Add padding to input_ids\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length # [23, 45, 67, 89] + [2, 2, 2] (if pad_token_id is 2)\n",
    "        \n",
    "        # Add 0s to attention mask for paddin\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        ## [1, 1, 1, 1] + [0, 0, 0] # 0 means: \"ignore this padding token\"\n",
    "        \n",
    "        # Add -100 to labels for padding\n",
    "        labels = labels + [-100] * padding_length\n",
    "        ## [-100, -100, 67, 89, 12, 4, 2] + [-100, -100, -100]\n",
    "        # -100 for pad tokens too\n",
    "        #We only want model to learn to predict the response, not:\n",
    "        #The prompt (first -100s)\n",
    "        #The padding (last -100s)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8, #Rank of the LoRA adaptation matrices\n",
    "    lora_alpha=16, #Scaling factor for the LoRA layers\n",
    "    target_modules=['q_proj', 'v_proj'], ## For LLaMA models, typically targets attention layers: # 'q_proj': Query projection,  'v_proj': Value projection\n",
    "    lora_dropout=0.05, #Dropout probability for LoRA layers\n",
    "    bias=\"none\", #How to handle bias terms\n",
    "    task_type=\"CAUSAL_LM\" ## \"CAUSAL_LM\": For autoregressive/generative tasks, # Other options like \"SEQ_2_SEQ_LM\" for different architectures\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "if device == \"cuda\":\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "# Create PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training arguments based on device\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./alpaca-tuned-model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1 if device == \"cpu\" else 4,\n",
    "    gradient_accumulation_steps=8 if device == \"cpu\" else 4,\n",
    "    learning_rate=2e-4,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",  # Changed from \"epoch\" to \"no\"\n",
    "    remove_unused_columns=False,\n",
    "    fp16=device == \"cuda\",\n",
    "    no_cuda=device == \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and process dataset => Creates a Hugging Face dataset from a list of dictionaries (JSON format data)\n",
    "dataset = Dataset.from_list(training_data)\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Print training device information\n",
    "print(f\"Training will be performed on: {device}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7732563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "peft_model.save_pretrained(\"./alpaca-tuned-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
