{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39adbea4",
   "metadata": {},
   "source": [
    "Reference -- https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0d87f",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we’ll explore exactly what happens in the tokenization pipeline.\n",
    "\n",
    "In NLP tasks, the data that is generally processed is raw text. Here’s an example of such text:\n",
    "\n",
    "```\n",
    "Jim Henson was a puppeteer\n",
    "```\n",
    "\n",
    "However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That’s what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.\n",
    "\n",
    "Let’s take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93761f",
   "metadata": {},
   "source": [
    "### Word-based\n",
    "\n",
    "The first type of tokenizer that comes to mind is word-based. It’s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d88d1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ff0e8",
   "metadata": {},
   "source": [
    "### Character-based\n",
    "\n",
    "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "\n",
    "+ The vocabulary is much smaller.\n",
    "\n",
    "+ There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "\n",
    "But here too some questions arise concerning spaces and punctuation.\n",
    "\n",
    "This approach isn’t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it’s less meaningful: each character doesn’t mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.\n",
    "\n",
    "Another thing to consider is that we’ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f27476",
   "metadata": {},
   "source": [
    "### Subword tokenization\n",
    "\n",
    "To get the best of both worlds, we can use a third technique that combines the two approaches: subword tokenization.\n",
    "\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "For instance, “annoyingly” might be considered a rare word and could be decomposed into “annoying” and “ly”. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of “annoyingly” is kept by the composite meaning of “annoying” and “ly”.\n",
    "\n",
    "Here is an example showing how a subword tokenization algorithm would tokenize the sequence “Let’s do tokenization!“:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33842cb6",
   "metadata": {},
   "source": [
    "## Loading and saving\n",
    "\n",
    "Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n",
    "\n",
    "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6347d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb92e7d92124e2a88eeb47d44fc33db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7235a442797457b9f40b5b9162cfef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bd3d5",
   "metadata": {},
   "source": [
    "Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74dcabd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c716617e8f4e6e8f351c3af05b78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cbd29f",
   "metadata": {},
   "source": [
    "We can now use the tokenizer as shown in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acefe950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934fb5b",
   "metadata": {},
   "source": [
    "Saving a tokenizer is identical to saving a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d0a5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../BERT_TOKENIZER/tokenizer_config.json',\n",
       " '../BERT_TOKENIZER/special_tokens_map.json',\n",
       " '../BERT_TOKENIZER/vocab.txt',\n",
       " '../BERT_TOKENIZER/added_tokens.json',\n",
       " '../BERT_TOKENIZER/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../BERT_TOKENIZER\", exist_ok=True)\n",
    "tokenizer.save_pretrained(\"../BERT_TOKENIZER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728af497",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Translating text to numbers is known as **encoding**. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n",
    "\n",
    "As we’ve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called **tokens**. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
    "\n",
    "The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a **vocabulary**, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f924b",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e0497d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc341d93",
   "metadata": {},
   "source": [
    "This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That’s the case here with transformer, which is split into two tokens: *transform* and *##er*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a9f2d",
   "metadata": {},
   "source": [
    "### From tokens to input IDs\n",
    "\n",
    "The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b6291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916efa9",
   "metadata": {},
   "source": [
    "These outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen earlier in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b75e8",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab715d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742670a",
   "metadata": {},
   "source": [
    "Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a607788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
